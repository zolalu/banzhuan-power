---
title: 关于Agent的日常思考
date: 2025-05-27T00:33:36.320Z
lastModified: 2025-05-27T01:34:55.447Z
tags: AI Agent
coverImage: https://images.unsplash.com/photo-1591696331111-ef9586a5b17a?w=800&h=400&fit=crop
---

==不论是做通用Agent还是垂直Agent，产品的起点通常很相似：用户的问题是什么？希望Agent帮他解决什么痛点？
技术测试能让工程和模型跑得稳，但推动产品进化的，永远是那些用户场景里具体、难缠、甚至有点烦人的需求。好产品不是在实验室里长出来的，而是能帮助用户偷懒，在用户的反馈和吐槽里打磨出来。基于真实的用户案例，再回头审视Agent产品的发展，才能看清它到底好不好用，卡在哪儿，怎么更好用。
现阶段好的Agent产品一定有一个阶段目标和要解决的问题，不应该是个只会雕花网页的大玩具。先听懂用户的难题，再倒推Agent该怎么长、怎么落地，是产品经理在其中需要发挥的核心作用。==

> Dia：如果你想做出真正“有温度”的Agent产品，建议多和一线用户深聊，问他们“如果有个理想的Agent，你最希望它帮你做什么”，再结合这些顶尖公司的案例，反推Agent的能力边界和演进路径。你会发现，最打动人的Agent，往往不是最炫技的，而是最懂用户“偷懒、偷巧、偷心思”的那一个。

# 产品思考 ：重要的是用户场景
AI生态会是啥样的 ？
元宝x微信在图谱中的潜力
![Agent日常思考-AI生态.png](img_1748308891397_Agent日常思考-AI生态.png)

AI与人的交互方式是什么样？
产品设计需要同时考虑AI和用户。

![Agent日常思考-AI与人交互.png](img_1748308904502_Agent日常思考-AI与人交互.png)

1. 相比生活助手场景，工作场景对智能的要求更高。好的Agent应用场景不会只限于生活助手，而是能把AI自身的效率发挥到极致，成为和人类员工并存的数字员工。
2. 就像我对Agent的期待不止是能帮我制定旅行计划、选商品购物；还能帮我解决工作上的实际问题，比如让它深入介入我的工作流，包括但不限于积极主动提供帮助、制定计划、利用我的笔记库&公司内部数据作为上文等等。工具的复用性是其中的关键
3. 对用户来说，最省事的方式就是只需表达意图，多数操作由Agent搞定，Agent会成为用户和系统之间的“接口”。也因此，只要产品还跑在网页或APP上，聊天框这种交互形态还会持续存在。这是AI这种新的智能形式，对现有网页/APP生态的迁就。
4. AI应用需要找到与AI协作的方向盘，比较好的抓手是做某个场景的All in one，解决一部分用户的问题，把用户刚需的场景做深做透，目前融入我日常生活的产品有：Cursor；Dia；Chatbot；Granola；Deep research；Manus（还没到）

5. 目前为止，我们还需要哪些技术突破？
- 大方向是端到端，但缺少一个 agent 的互联网：传统互联网环境本身需要一次重构，不再依赖浏览器操作，而是要为AI重新搭建一个适合机器的浏览环境，让 agent 直接执行任务。

- AI 的执行能力还有限，可能调用 50 个工具就到极限了。未来我们要解决的是：如何让 agent 在未知环境下自主调用上千个工具，完成复杂任务。这里不止涉及工程问题，即工具使用需要更加灵活/MCP 之类的协议需要持续优化，还有 LLM 本身的记忆和规划能力。

- Agent 对于多模态的理解能力，不只限于文字，而应该更多形式地理解需求，这里包括 Agent 对于环境的感知能力（比如 GUI 交互能不能做好），个人在互联网环境中留下来的全部个性化数据等等（比如谷歌自己在 gemini 给用户连接浏览历史、地图）

# 技术思考 ：认知-执行-交付

个人认为 Agent 首先都是一个 Coding Agent，Coding / File / Shell use 是 Agent 的基础能力，在这个基础能力给与 Agent 不同的职责（System Prompt）和 Tool（Environment），才派生出了不同的Agent。

## 一、技术场景
从技术角度来看，Agent的应用场景可以拆分为三个主要方向：
- 1. 研究写作：对应的场景是Deep Research，侧重信息搜索、深度分析和内容生成。
2. 代码交付：不仅仅是代码生成，更关注实际业务需求，需要引入更多后端逻辑相关的项目。这也是Agent应具备的基础能力之一。
3. 实用任务：以提升效率为目标，聚焦于解决工作和生活中的各类数字员工任务。在这一过程中，Agent通常需要结合搜索、写作、网页浏览、代码处理、文件管理等多种工具进行复合规划。对于难度较高的任务，往往还需要专家级人类进行结果评估，并且人类与Agent之间会有多轮交互。这一领域被认为是Agent最有前景和价值的应用场景。

## 二、任务特性
- 长程：任务流程长、步骤多，要求AI具备持续推理和多步规划的能力。典型场景包括大型应用或游戏开发、需要多轮搜索与整合的报告撰写、超长篇幅的创意写作等，这些任务无法通过单次问答完成，考验Agent的前序记忆和规划能力。
- 复合：任务往往不是单一类型，而是多种任务的组合，需要Agent能够灵活调度和整合多种工具（如搜索、写作、代码、文件处理等），并提出创新性甚至超越专家的解决方案。这类任务通常超出了基础模型的能力范畴，可参考HLE等高难度专业任务。
- 开放：任务的问题边界通常不明确，答案也不是唯一，强调创新性和适应能力。
- 交互：任务推进过程中需要人机多轮互动，用户与Agent共同协作完成任务。具体表现为：可操作的虚拟环境（如网页、沙盒、本地系统，例：Mind2web）；模型需要能够理解人类指令，并根据用户反馈动态调整任务规划，这也是长程任务的典型特征。
- 多模态：任务的输入和输出不限于文本或代码，还包括图像、音频等多种模态的信息处理。

## 三、Agent的评测标准发生了什么变化？

构成Agent工程稳定性的要素有：Benchmark, LLM, Prompt, Tool, Memory

### 01 Benchmark：定义和评估 Agent 的表现
这个部分的重点是任务/数据构造和具体结果的质量评估。

#### 任务/数据构造
1. 搜索&构造User Case（学科研究、业务开发、实际工作任务）
- 来源：不同学科、职业、业务场景的真实复杂任务。
- 研究：找不同学科领域的phd，实际研究题目
- code：找各种业务类型的开发，实际交付需求/项目。现在的code主要是前端页面评估比较多，建议更多引入偏后台逻辑的项目。
- 实用任务：找不同职业，实际工作和生活要求

2. 参考学术测试集与开源Benchmark
- 已有开源框架可作为参考，如专家问答（HLE）、代码任务（SWE、SWEET）、实用任务（GAIA）等。
- 典型Benchmark案例：
	- Mind2Web、WebJudge：专注网页操作任务，通过模型拆解关键点，结合截图和说明，评估任务完成度，并引入人工评测对齐。
	- Paperbench：基于权威rubrics，原作者定义子任务节点及权重，采用二分类打分，结合JudgeEval与人类评判对齐。
	- SWE、HLE、GAIA、SimpleQA、SWEET-RL、AgentBench、Multi-agent bench等，分别覆盖代码、专家问答、多智能体协作等多种类型，均有开源实现或论文可供参考。

友情感谢Deep research 帮我读论文，附一些调研报告：
[ChatGPT：Agent评估方法与Benchmark体系调研](https://chatgpt.com/s/dr_6826a09cb21c819187d6f23c8e59aed7)
[Minimax：Agent 测试评估方法调研报告](https://agent.minimax.io/share?share_chat_id=269479085187143)
[Manus：gent Testing and Evaluation Methods Research](https://manus.im/share/rTmfXWtzIILUI2w58YWUSi?replay=1)

#### 效果评估
##### 交付内容质量评估
1. 有标答任务：提前构造标准答案，评测时直接对比结果与标准答案，便于量化打分。
2. 无标答任务：采用同类型任务的基础标准（如代码运行准确、写作信息密度和风格），并结合更高阶的专家评估标准（如网页美观度、报告数据准确性），根据具体任务进一步细化。
3. 主要任务类型及其评分逻辑
- 研究写作任务
评估方式：参考真实工作场景下的专家评估。
核心维度：相关性、写作质量（逻辑性、信息密度、风格、格式）、领域标准（事实验证、数据可靠性、时效性、对决策的指导意义）。
特殊要求：涉及搜索和网页浏览的，需保证信息来源固定或统一检索，确保评测公平性。

- 代码任务
评估方式：参考实际开发验收标准，考察功能完成度、产品验收点、代码质量、运行效果。
效果指标：通过构造一批QA case评估需求完成度/成功率，结合产品或设计验收点，评估需求满足度。
质量细则（以TypeScript为例）：类型系统利用、组件设计与复用、状态管理、性能优化、用户体验、现代框架特性、响应式与适配、文档与注释、代码量与复杂度（过少或过于简单会被扣分）。
若后续还考虑引入人类多轮交互，可参考人机协同框架（sweetrl），但是由于除了代码任务还有更多目标更开放的任务，这个过程可能会更加复杂。

- 实用任务
评估方式：结合多轮交互和开放目标，参考实际工作场景进行综合评估。（这部分是目前相对欠缺的）

##### 执行性能评估
1. 任务效率：关注完成任务所需的时间和资源消耗（如推理、内存等）。
2. 鲁棒性：多次重复执行任务时，结果的稳定性和成功率。
3. 记忆与保持能力：测试Agent对上文的记忆和持续保持能力（目前评估方法尚需完善）。
4. gold标准与专家参与：
- 复杂任务需引入领域专家评分，基础标准与高阶标准并行。
- 多轮任务更贴合真实工作场景，评测时需模拟人类反馈与环境交互。

> Dia：简而言之，评估体系既要看结果“对不对”“好不好”，也要看Agent“干得快不快”“稳不稳”“能不能持久”，复杂任务还需专家把关和模拟真实互动。

### 02 LLM ：熟悉并了解不同 LLM 的特点
- Claude 擅长 Coding, Tool Use, UI Design。但搜索收集信息时面窄，写报告也比较简略。上下文短（20 W），不是特别适合做 Research。速度慢成本高，审美高级。
- Claude Origin Tool: Shell / File Editor（str_replace_editor），原生 Tool 是效果提升的一个关键
- Gemini 擅长 Research 和 Report Writing，上下文长（100 W）。 Coding 和 Tool use 不稳定，有时候 80 分，有时候 60 分，有时候 0 分。速度快成本低，审美差。

### 03 Prompt ：设计system prompt与响应user case
- System：明确不同Agent的职责和定位，比如它是“打杂的”还是“专家型顾问”。
- User prompt ：明确用户的query和意图。

### 04 Tool : Computer For Agent
- 操作系统级别：文件编辑器、Shell，和人类程序员一样，vim+shell能解决不少问题。
- Web能力：curl、搜索、爬取，甚至直接用无头浏览器或真浏览器。
- API/MCP：接入垂直数据源，比如Google Map、Booking、Yahoo Finance
- Agent Infra / Saas: 沙盒、BaaS、supabase、专为Agent设计的浏览器/搜索，API/MCP、安全、记忆……

### 05 Memory ：智能+工程
在模型智能没有提升之前，目前工程上需要评估的是什么信息该让Agent记住，什么可以扔掉。方法包括：
- 明确Agent需要掌握哪些信息：比如文件内容、其他Agent的记忆、以及自身历史记忆的保留范围。
- 会话中的Prompt干预：在Agent运行过程中，可以通过插入提示来引导Agent。例如，当Agent陷入死胡同时，要求其进行总结和反思；或者提供更多环境信息，帮助其决定下一步行动。干预对象可以是用户、工具，甚至是Agent本身
- 设定人设和记忆：可以为AI设定特定身份和记忆，让它“假装”自己曾经完成过某些任务，以此影响后续行为。
- Context Window管理：当上下文窗口达到容量上限时，需要评估哪些内容应该被总结、哪些信息需要保留，以保证Agent持续有效地工作。

### Agent or Workflow： 在具体实践中并非替代关系
- 确定性的走 Workflow：例如网页的爬取总结，处理各种反爬，超长文件，Media 等，往往封装为一个 Tool（workflow/sub-agent），供 Agent 直接使用。
- 针对具体问题，需要思考和创造性的走 Agent。

  

更多材料参考：[Google Cloud 600+ AI Agent真实案例](https://cloud.google.com/transform/101-real-world-generative-ai-use-cases-from-industry-leaders)（涵盖To B与TO C，按行业和 Agent类型分层），[36 Real-World Examples of AI Agents](https://botpress.com/blog/real-world-applications-of-ai-agents#8-multi-agent-systems)；[Make product management fun again with AI agents](https://www.lennysnewsletter.com/p/make-product-management-fun-again)